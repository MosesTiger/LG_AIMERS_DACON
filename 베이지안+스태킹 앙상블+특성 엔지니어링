import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import f1_score, confusion_matrix, classification_report
from sklearn.feature_selection import SelectFromModel, mutual_info_classif
from imblearn.over_sampling import SMOTE
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer
from mlxtend.classifier import StackingCVClassifier
import featuretools as ft
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.wrappers.scikit_learn import KerasClassifier

# 설정
ROOT_DIR = "."
RANDOM_STATE = 42

# 1. 데이터 로드 및 기본 탐색
def load_and_explore_data(file_path):
    data = pd.read_csv(file_path)
    print(f"Data loading complete. Shape: {data.shape}")
    
    # 기본 통계 정보
    print(data.describe())
    
    # 결측치 정보
    missing_data = data.isnull().sum()
    print("Missing data:\n", missing_data[missing_data > 0])
    
    # 타겟 변수 분포 (훈련 데이터의 경우)
    if 'target' in data.columns:
        print("Target distribution:")
        print(data['target'].value_counts(normalize=True))
        
        # 타겟 변수 분포 시각화
        plt.figure(figsize=(8, 6))
        sns.countplot(x='target', data=data)
        plt.title('Target Distribution')
        plt.savefig('target_distribution.png')
        plt.close()
    
    return data

# 2. 자동화된 특성 엔지니어링
def automated_feature_engineering(data):
    es = ft.EntitySet(id="data")
    es = es.add_dataframe(
        dataframe_name="data",
        dataframe=data,
        index='Set ID',  # 고유 식별자 칼럼명을 적절히 변경하세요
        time_index=None  # 시간 정보가 있다면 해당 칼럼명으로 변경하세요
    )
    
    feature_matrix, feature_defs = ft.dfs(
        entityset=es,
        target_dataframe_name="data",
        trans_primitives=['add_numeric', 'multiply_numeric'],
        max_depth=2,
        features_only=False
    )
    
    return feature_matrix

# 3. 데이터 전처리
def preprocess_data(data, is_train=True):
    # 'OK' 값을 np.nan으로 변환
    data = data.replace({'OK': np.nan})
    
    # 결측치가 80% 이상인 열 제거
    threshold = 0.8
    data = data.dropna(thresh=int((1-threshold) * len(data)), axis=1)
    
    # 수치형 컬럼과 범주형 컬럼을 분리
    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
    
    if is_train and 'target' in categorical_cols:
        categorical_cols.remove('target')
    
    # 전처리 파이프라인 설정
    numeric_transformer = Pipeline(steps=[
        ('imputer', KNNImputer(n_neighbors=5)),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])
    
    return preprocessor, data

# 4. 딥러닝 모델 정의
def create_nn_model(input_dim):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# 5. 스태킹 앙상블 모델 정의
def create_stacking_ensemble(X, y):
    base_models = [
        ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),
        ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),
        ('xgb', XGBClassifier(random_state=RANDOM_STATE)),
        ('lgbm', LGBMClassifier(random_state=RANDOM_STATE)),
        ('nn', KerasClassifier(build_fn=lambda: create_nn_model(X.shape[1]), epochs=100, batch_size=32, verbose=0))
    ]
    
    stacking_model = StackingCVClassifier(
        classifiers=[model for _, model in base_models],
        meta_classifier=LogisticRegression(),
        use_probas=True,
        cv=5,
        random_state=RANDOM_STATE
    )
    
    return stacking_model

# 6. 베이지안 최적화를 이용한 하이퍼파라미터 튜닝
def optimize_hyperparameters(X, y):
    search_spaces = {
        'rf__n_estimators': Integer(100, 1000),
        'rf__max_depth': Integer(5, 30),
        'gb__n_estimators': Integer(100, 1000),
        'gb__learning_rate': Real(1e-3, 1.0, prior='log-uniform'),
        'xgb__n_estimators': Integer(100, 1000),
        'xgb__learning_rate': Real(1e-3, 1.0, prior='log-uniform'),
        'lgbm__n_estimators': Integer(100, 1000),
        'lgbm__learning_rate': Real(1e-3, 1.0, prior='log-uniform'),
    }
    
    model = create_stacking_ensemble(X, y)
    
    optimizer = BayesSearchCV(
        model,
        search_spaces,
        n_iter=50,
        cv=5,
        n_jobs=-1,
        random_state=RANDOM_STATE
    )
    
    optimizer.fit(X, y)
    
    return optimizer.best_estimator_

# 메인 실행 코드
if __name__ == "__main__":
    # 데이터 로드 및 탐색
    train_data = load_and_explore_data(os.path.join(ROOT_DIR, "train.csv"))
    test_data = load_and_explore_data(os.path.join(ROOT_DIR, "test.csv"))

    # 자동화된 특성 엔지니어링
    train_data_fe = automated_feature_engineering(train_data)
    test_data_fe = automated_feature_engineering(test_data)

    # 데이터 전처리
    preprocessor, train_data_processed = preprocess_data(train_data_fe)
    _, test_data_processed = preprocess_data(test_data_fe, is_train=False)

    # 클래스 라벨을 숫자로 변환
    label_encoder = LabelEncoder()
    train_data_processed['target'] = label_encoder.fit_transform(train_data_processed['target'])

    # 특성 변환
    X = preprocessor.fit_transform(train_data_processed.drop('target', axis=1))
    y = train_data_processed['target'].values

    # 학습 데이터와 검증 데이터 분리
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)

    # SMOTE를 사용한 오버샘플링
    smote = SMOTE(random_state=RANDOM_STATE)
    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

    # 베이지안 최적화를 이용한 하이퍼파라미터 튜닝 및 모델 학습
    best_model = optimize_hyperparameters(X_resampled, y_resampled)

    # 검증 데이터에 대한 성능 평가
    val_pred = best_model.predict(X_val)
    print("Validation Performance:")
    print(classification_report(y_val, val_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_val, val_pred))

    # 테스트 데이터 예측
    X_test = preprocessor.transform(test_data_processed)
    test_pred = best_model.predict(X_test)

    # 예측된 라벨을 원래 문자열로 변환
    test_pred = label_encoder.inverse_transform(test_pred)

    # 제출 파일 작성
    submission = pd.DataFrame({"Set ID": test_data['Set ID'], "target": test_pred})
    submission.to_csv(os.path.join(ROOT_DIR, "submission.csv"), index=False)
    print("Submission file created successfully.")

    # 특성 중요도 시각화 (가능한 경우)
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = best_model.feature_importances_
        feature_names = preprocessor.get_feature_names_out()
        
        plt.figure(figsize=(12, 8))
        sns.barplot(x=feature_importance, y=feature_names)
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        plt.close()
